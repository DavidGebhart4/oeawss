{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c80f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import PIL, urllib\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9d4d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "document.title = \"My OEAW SummerSchool App\";\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "document.title = \"My OEAW SummerSchool App\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccf6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_url = 'https://upload.wikimedia.org/wikipedia/commons/2/24/Blue_Tshirt.jpg'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa00a1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0c84c899e446739305c927955b10c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<h1>\\nMy OEAW Summer School App\\n</h1>\\n<p style=\"max-width: 500px;\">\\nThis app takes a url to aâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.HTML('''\n",
    "<h1>\n",
    "My OEAW Summer School App\n",
    "</h1>\n",
    "<p style=\"max-width: 500px;\">\n",
    "This app takes a url to a .jpg or .png image.\n",
    "Converts it into a grey scale 28x28 image and passes\n",
    "it through neural network trained on FashionMNIST dataset.\n",
    "The predicted class is than simply displayed as a text.\n",
    "Type in an image url and press enter.\n",
    "</p>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bda0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # input layer\n",
    "        self.linear1 = nn.Linear(784, 1024)\n",
    "        #hidden layers\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.linear3 = nn.Linear(2048, 1024)\n",
    "        self.linear4 = nn.Linear(1024, 512)\n",
    "        #output layer\n",
    "        self.linear5 = nn.Linear(512, 10)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.2)  # values changed in exercise 3\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.dropout4 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "        \n",
    "        x = self.linear4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.selu(x)\n",
    "        \n",
    "        x = self.linear5(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d1fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    # CNN architecture (two conv layers followed by three fully connected layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d457de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ValueError('Expected tensor to be a tensor image of size (C, H, W). Got tensor.size() = torch.Size([1, 1, 28, 28]).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(mt=widgets.RadioButtons(\n",
    "    options=['CNN', 'NN'],\n",
    "    value='CNN',\n",
    "    description='Model:',\n",
    "    disabled=False\n",
    "))\n",
    "def f(mt):\n",
    "    if mt == 'CNN':\n",
    "        model = ConvNet()\n",
    "        model_path = 'trained_cnn.pt'\n",
    "        shape = (28,28)\n",
    "    else:\n",
    "        model = NeuralNet()\n",
    "        model_path = 'trained_model.pt'\n",
    "        shape=(784,)\n",
    "\n",
    "    # Load the desired model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device));\n",
    "    model.eval()\n",
    "    \n",
    "    @interact(\n",
    "        url=widgets.Text(\n",
    "            value=default_url,\n",
    "            placeholder='Type in an image URL.',\n",
    "            description='Image URL:',\n",
    "            disabled=False,\n",
    "            continuous_update=False))\n",
    "    def g(url):\n",
    "        try:\n",
    "            # Load and transform the image\n",
    "            img_arr = PIL.Image.open(\n",
    "                urllib.request.urlopen(url)).convert('L').resize((28,28))\n",
    "\n",
    "            # Plot the transformed image\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(img_arr, cmap='gray', aspect='equal')\n",
    "            plt.title(f'Converted into grayscale {img_arr._size} image')\n",
    "            plt.axis('off')\n",
    "            plt.show()   \n",
    "\n",
    "            # Convert the image to tensor\n",
    "            data = torch.tensor(np.array(img_arr), dtype=torch.float32).view(1, 1, *shape)\n",
    "            data = transforms.Normalize((0.1307,), (0.3081,))(data)\n",
    "            \n",
    "            # Pass it through the model and display the result\n",
    "            with torch.no_grad():\n",
    "                display(FashionMNIST.classes[model(data).argmax().item()])\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663b0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
